{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVn_dUcN-MPL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11d6ac7a-87dc-4358-c24d-66f1e1b79634"
      },
      "source": [
        "# NEURAL NETWORKS DO NOT DO DATA CLEANING!\n",
        "\n",
        "# IF TF prefers channels _Last-> we need to arrange our images before passing to NN\n",
        "# THEANO -> Channels_first -> WE need to arrange our images\n",
        "\n",
        "\n",
        "# in our NLP problem -> preprocessed sentences were analyzed\n",
        "# in our CV problems-> images are preprocessed \n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgQWq65r_Hgv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "2a2a4759-eab3-4349-9f54-49ac926e1df5"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "img = cv2.imread('000003.jpg')\n",
        "img_resized = cv2.resize(img, (200,200)) # SMALL DISTORTION are WELCOME! \n",
        "cv2_imshow(img_resized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=200x200 at 0x7F95509F30F0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_UrZDhr_-76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "870bd99d-288d-43d8-9cce-443cf46f907d"
      },
      "source": [
        "img_resized[0] # EACH element is a LIST of 3 elements -> CHANNEL value-> 3 channels\n",
        "img_resized.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 200, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkTTK6ZOAR1Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13cd3e31-dc3f-4516-de86-18f2cc82c392"
      },
      "source": [
        "img_resized[5][0] # 5th row, 0th column\n",
        "\n",
        "# uint8 -> each channel is unsigned (no negative signs) and 8 bit in size "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([255, 255, 255], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q0GHkfNA8MR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "96521074-c2d5-4093-b496-be43aaeb3133"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import numpy as np\n",
        "from keras import backend # backend -> tf/theano etc\n",
        "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Sequential # no parallel, no cycles \n",
        "\n",
        "# TF can create all = DAG, DCG, IAG, ICG "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BVCnCrFTxM5"
      },
      "source": [
        "# SUCH networks can be VERY large and usually only 1 instance runs \n",
        "# otherwise RAM will be cluttered and all will crash! (OS+model+harddisk)\n",
        "\n",
        "# NO MORE THAN 1 INSTANCE of such programs should be RUNNING! \n",
        "\n",
        "# SINGLETON -> class which can have ONLY 1 instance -> TASK MANAGER! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz1VGDNmV65s"
      },
      "source": [
        "\n",
        " # the method declared after @st... will be STATIC\n",
        "  # ONLY 1 copy of it will RUN from the beginning till the end of program\n",
        "  # STATIC keyword -> STATIC MEANS ALWAYS PRESENT (Static CURRENT!)\n",
        "\n",
        "  # DEPTH = Channel depth = channel dimension = no. of channels \n",
        "  #                       \n",
        "# KERAS needs us to arrange -> IMAGE STRUCTURE \n",
        "    # Tensorflow -> CHANNEL_LAST -> (height, width, depth)\n",
        "    # Theano -> CHANNEL_FIRST -> (depth, height, width)\n",
        "    # https://machinelearningmastery.com/a-gentle-introduction-to-channels-first-and-channels-last-image-formats-for-deep-learning/\n",
        "\n",
        "\n",
        "# PandaVGG = ((Conv + ReLU + BN) + MP + Dropout + ( (Conv + ReLU + BN) + MP + Dropout)XN\n",
        "#             + (Flatten + Dense(ReLu) + Dense(SoftMax))\n",
        "\n",
        "class PandaVGG:\n",
        "  @staticmethod\n",
        "  def build( height, width, depth, classes) :\n",
        "  \n",
        "    # let's assume we are on TF, else we will switch to Theano \n",
        "    inputShape = (height, width, depth)\n",
        "    channel_dim = -1  # last element -> CHANNELS_LAST \n",
        "    if backend.image_data_format() == 'channels_first':\n",
        "      inputShape = (depth, height, width)\n",
        "      channel_dim = 1\n",
        "    # { data, representation } => [data,representation] or [representation, data]\n",
        "    #  { images, channels}     => channels_last or channels_first \n",
        "    # data -> 2 d images -> height [row], width [columns] \n",
        "\n",
        "    HP_block1_conv_dim = 32    \n",
        "    HP_small_pattern = (3,3)\n",
        "    HP_block2_conv_dim = 64\n",
        "    HP_block3_conv_dim = 128\n",
        "    HP_block4_conv_dim = 256\n",
        "    HP_block5_dense_dim = 1024\n",
        "    HP_large_pattern = (2,2)\n",
        "    HP_dropout_type1 = 0.25\n",
        "    HP_dropout_type2 = 0.50\n",
        "    \n",
        "    model = Sequential()\n",
        "# PandaVGG = (Conv + ReLU + BN) + MP + Dropout + ((Conv + ReLU + BN)X2 + MP + Dropout) X N\n",
        "#             + (Flatten + Dense(ReLu) + Dense(SoftMax))\n",
        "\n",
        "    # block1 starts-> (Conv + ReLU + BN) + MP + Dropout \n",
        "    model.add(Conv2D(HP_block1_conv_dim, HP_small_pattern, padding='same', input_shape=inputShape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=channel_dim)) # our data needs to be normalized, not our channels!!!\n",
        "    model.add(MaxPooling2D(pool_size=HP_small_pattern))\n",
        "    model.add(Dropout(HP_dropout_type1))\n",
        "    # block1 complete \n",
        "\n",
        "    #block2 starts -> (Conv + RelU + BN) X2   + Compress + Drop \n",
        "    model.add(Conv2D(HP_block2_conv_dim,HP_small_pattern, padding='same' ))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=channel_dim))\n",
        "    model.add(Conv2D(HP_block2_conv_dim,HP_small_pattern, padding='same' ))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=channel_dim))\n",
        "    model.add(MaxPooling2D(pool_size=HP_large_pattern))\n",
        "    model.add(Dropout(HP_dropout_type1))\n",
        "    # BLock 2 ends \n",
        "\n",
        "    #Block3 starts\n",
        "    model.add(Conv2D(HP_block3_conv_dim,HP_small_pattern, padding='same' ))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=channel_dim))\n",
        "    model.add(Conv2D(HP_block3_conv_dim,HP_small_pattern, padding='same' ))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=channel_dim))\n",
        "    model.add(MaxPooling2D(pool_size=HP_large_pattern))\n",
        "    model.add(Dropout(HP_dropout_type1))\n",
        "    # block3 ends\n",
        "\n",
        "    #Block4 starts\n",
        "    model.add(Conv2D(HP_block4_conv_dim,HP_small_pattern, padding='same' ))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=channel_dim))\n",
        "    model.add(Conv2D(HP_block4_conv_dim,HP_small_pattern, padding='same' ))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=channel_dim))\n",
        "    model.add(MaxPooling2D(pool_size=HP_large_pattern))\n",
        "    model.add(Dropout(HP_dropout_type1))\n",
        "    # block4 ends\n",
        "\n",
        "    # final block5- classification/prediction\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(HP_block5_dense_dim))\n",
        "    model.add(Activation('relu'))\n",
        "    # optional: Batch Normalization and Dropout -> to avoid overfitting \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(HP_dropout_type2))\n",
        "    model.add(Dense(classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    # final block ends\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsqHr2JaObEn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c96a515-e151-42ec-c3ec-f30019c18098"
      },
      "source": [
        " # minimum image sizes that NN can clearly identify\n",
        "# black & white images                          -> 28X28 \n",
        "# multi-channels images (3 channels in our case)-> 96X96 (3 X (32,32)) \n",
        "\n",
        "#model = PandaVGG.build(96, 96, 3, 3)\n",
        "model.summary() \n",
        "\n",
        "# 32 filters -> each filter is 3X3 -> PER CHANNEL\n",
        "# 1 filter = [ [w11,w12,w13],[w21,w22,w23],[w31,w32,w33]] -> 9 weights \n",
        "# 32 X 9 = 288 -> 1 channel\n",
        "\n",
        "# 288 X 3 = 864 weights\n",
        "# 32 filters generated, each with 1 bias\n",
        "# weights + bias = 864 + 32 = 896\n",
        "\n",
        "# EXPORT the built model -> right approach \n",
        "# next file-> train.py -> import the model "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 96, 96, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 96, 96, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 96, 96, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              4195328   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 3075      \n",
            "=================================================================\n",
            "Total params: 5,365,123\n",
            "Trainable params: 5,363,267\n",
            "Non-trainable params: 1,856\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmLr0E8mU9Pa"
      },
      "source": [
        "# data_clean.py \n",
        "# in real life, should have been a separate file\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from sklearn.preprocessing import LabelBinarizer # Label encoding, 1-hot encoding, multi-encoding\n",
        "# LABEL binarizer is a 1-hot encoded MATRIX \n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import imutils\n",
        "from imutils import paths\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx3uMIvDVQZc"
      },
      "source": [
        "# if this was a User Facing software or web/mobile app, we would have been using a UI\n",
        "HP_dataset = 'data'\n",
        "HP_model_path = 'bin/model/myModel.h5'\n",
        "HP_binarized_labels = 'bin/labels/labels.bin'\n",
        "HP_metrics_storage = 'eval'\n",
        "HP_test_dataset = 'test'\n",
        "HP_epoch = 100\n",
        "HP_init_lr = 1e-3 # learning_rate = 0.001\n",
        "HP_batch_size = 32\n",
        "HP_image_dim = (96,96,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00xIhEifYo26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "1fab3b3d-f366-4368-eb56-98651d2d07f6"
      },
      "source": [
        "data = []\n",
        "labels = [] \n",
        "# read all images\n",
        "all_images = sorted(list(paths.list_images(HP_dataset)))\n",
        "all_images[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/dogs/00001.png',\n",
              " 'data/dogs/00005.png',\n",
              " 'data/dogs/00087.png',\n",
              " 'data/dogs/00120.png',\n",
              " 'data/dogs/00132.png']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNwYb47uZoVe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7bb7f06-16d8-44fd-c034-f73b34d96097"
      },
      "source": [
        "random.seed(42)\n",
        "random.shuffle(all_images)\n",
        "all_images[:5]\n",
        "print(len(all_images))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg-WQMLhaAQg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec0bb8b2-ac5c-4560-edf6-03e62c9679e1"
      },
      "source": [
        "import os\n",
        "for impath in all_images:\n",
        "  img = cv2.imread(impath)\n",
        "  resized = cv2.resize(img, (HP_image_dim[0],HP_image_dim[1]) )\n",
        "  imageData = img_to_array(resized)\n",
        "  data.append(imageData)\n",
        "  # extract label from filename (2nd last element) / \\\\ \n",
        "  label = impath.split(os.path.sep)[-2]\n",
        "  labels.append(label)\n",
        "\n",
        "print(len(labels))\n",
        "#print(data[0])\n",
        "# X and Y -> data and labels "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L9T9Y5Kh_tb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "87bfe92e-ba97-4407-a1fa-0952a89d0286"
      },
      "source": [
        "# CONVERT labels to MATRICES -> if input is matrix, output should be MATRIX!\n",
        "# data[element] -> Array of arrays \n",
        "labels = np.array(labels)\n",
        "lb = LabelBinarizer()\n",
        "binarized_labels = lb.fit_transform(labels)\n",
        "print(binarized_labels[0])\n",
        "print(binarized_labels[1])\n",
        "print(len(binarized_labels))\n",
        "\n",
        "# 1-hot encoding v/s          Label Binarizing \n",
        "# Inputs                      Labels (outputs)\n",
        "# creates new columns         creates 1-hot encoding in same element \n",
        "#                             [ [dog] , [pikachu], [shaktiman]]\n",
        "#                             4 -> 1 0 0  -> number's binary rep is used to gen Binarizers \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0]\n",
            "[0 0 1]\n",
            "471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSawpOWKjzR7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "35f34e28-9c57-43fc-9025-76d30896502a"
      },
      "source": [
        "binarized_labels[0].dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P22_m0oUkLwz"
      },
      "source": [
        "norm_data = np.array(data, dtype='float') / 255 \n",
        "trainx, testx, trainy, testy = train_test_split(norm_data,binarized_labels,test_size=0.2,random_state=42)\n",
        "# train and test data ready \n",
        "# Augmentation generators are logic for augmentation\n",
        "# APplication of augmentation is after this\n",
        "\n",
        "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1, height_shift_range=0.1,\n",
        "                         shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
        "\n",
        "\n",
        "# mathematical ops -> new_data = old_data * sin(7)\n",
        "\n",
        "# Regularization -> L1/L2 (ridge/lasso), Dropout, early stopping ,data augmentation \n",
        "# whenever working with images, we don;t want our model to overfit\n",
        "# FACE -> face directly looking at me\n",
        "# FACE -> 30 degrees to right, and still looking at me, we should be able to recognize it \n",
        "# as well!\n",
        "\n",
        "# we will generate ALL possible MUTATION/TRANSFORMATIONS of our images \n",
        "\n",
        "# data_cleaning.py ends here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ1LhNVj2ehZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ac403462-5bdf-4a04-c2d6-8fe3201ec3ff"
      },
      "source": [
        "print(len(trainx))\n",
        "print(len(trainy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "376\n",
            "376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPdqrbt_ycYl"
      },
      "source": [
        "# previous class is also correct, use any\n",
        "class CorrectPandaVGG: # model.fit(), model.predict() ; for them you need object\n",
        "  @staticmethod\n",
        "  def create(width, height, depth, classes): # TF was CHANNELS_LAST -> depth is mentioned as the last value, after w&h\n",
        "    model = Sequential()\n",
        "    inputShape = (height, width, depth) # the expected shape as per TF\n",
        "    # but what if this program is then run on some other framework? which follows channels_first instead?\n",
        "    chanDim = -1 # CHANNELS_LAST set as default \n",
        "    # now check if some other library except TF or CNTK is running\n",
        "    if backend.image_data_format() == \"channels_first\":\n",
        "      inputShape = (depth, height, width) # this chan_dim is a measure of DEPTH of data-> so makes sense only for multidimension data, such as image or video\n",
        "      chanDim = 1 # for Theano (if tf or CNTK is not running)\n",
        "    # Design our network\n",
        "    # FIRST BLOCK = Conv + ReLU + BN + MP + D\n",
        "    model.add(Conv2D(32,(3,3), padding='same', input_shape=inputShape)) # all keras backends compatible\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=chanDim)) # Normalization needs to be along axis- R,G and B! \n",
        "    # After BN, the important FILTERED features are available now as WEIGHTS \n",
        "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "    model.add(Dropout(0.25)) \n",
        "    # Learning Layers -> Conv + ReLU + BN\n",
        "    # that means, to increase the number of features, i need to repeat the Conv+ReLU+BN to generate more and more\n",
        "    # features\n",
        "    # MAX Pool is to compress, and Dropout -> randomly select features for partial learning \n",
        "    \n",
        "    # BLOCK 2 should have slightly more features, because we have compressed and DROPPED the data!!! \n",
        "    # we should generate more activity maps henceforth to compensate for data loss \n",
        "    # BLOCK 2 = (Conv->Relu->BN)X2 + MP + D\n",
        "    # LET X = Conv->Relu->BN\n",
        "    # Block 2 = 2 X + MP + D\n",
        "    # My network so far = Block 1 + Block 2\n",
        "    # FeatureClassification = Flatten + Dense(ReLU) + Dense(SoftMax)\n",
        "    # PandaVGG = X + MP + D +   ( 2 X + MP + D ) * no_of_times_you_want_to_repeat + FeatureClassification\n",
        "    # First X -> got activity map for 100% of data (X=Conv+Relu+BN)\n",
        "    # then Activity Maps are compressed and sent to subsequent layers (MaxPooling)\n",
        "    # Dropout to learn about only a section of data (Dropout)\n",
        "    model.add(Conv2D(64,(3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Conv2D(64,(3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2))) # SMALLER pool size-> finer features selected -> BIGGER compression\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # block 3\n",
        "    model.add(Conv2D(128,(3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Conv2D(128,(3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2))) # SMALLER pool size-> more data lost -> BIGGER compression\n",
        "    model.add(Dropout(0.25))\n",
        "    # block 4\n",
        "    model.add(Conv2D(256,(3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Conv2D(256,(3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2))) # SMALLER pool size-> more data lost -> BIGGER compression\n",
        "    model.add(Dropout(0.25))\n",
        "    #VGG has now become = block1 +block2+block3+block4 + FeatureClassification\n",
        "    # VGG= X(32)+MP(3,3)+D+2X(64)+MP(2,2)+D+2X(128)+MP(2,2)+D+2X(256)+MP(2,2)+D+Flatten+DenseRelu+DenseSoftMax\n",
        "    # Where X = Conv2D + ReLU + BN \n",
        "    # Our PandaVGG now has 32 layers\n",
        "\n",
        "    #FINAL Classification block\n",
        "    # ALL micro and macro features (FILTERsX Input = Activity Map) have been generated\n",
        "    # SO FLATTEN out to bring all axises into same DIMENSION\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "    # optional: EVEN more regularization\n",
        "    #model.add(BatchNormalization())\n",
        "    #model.add(Dropout(0.5))\n",
        "    #regularization over, Classify using SOFTMAX\n",
        "    model.add(Dense(classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZRMu-qaoqPo"
      },
      "source": [
        "# TRAIN.py should start! \n",
        "\n",
        "from keras.optimizers import Adam\n",
        "# or use the previous class, both are same\n",
        "model = CorrectPandaVGG.create(HP_image_dim[0], HP_image_dim[1], HP_image_dim[2],len(lb.classes_)))\n",
        "optimizer = Adam(lr=HP_init_lr, decay= HP_init_lr/HP_epoch)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# 1image = 10% shaktiman, 50% pikachu, 40% doggie (categorical_cross)\n",
        "# 1image = 0.3% shaktiman, 99.5% pikachu, 0.2% doggie (sparse_categorical_cross)\n",
        "\n",
        "# binary_crossentropy, sparse_categorical_crossentropy -> image classification\n",
        "# categorical_crossentropy -> object detection\n",
        "\n",
        "# DECAY -> when training is over, we want LR to be 0! \n",
        "# LR -> 0 means that perfect answer is reached \n",
        "# ADAPTIVE LEARNING-> inc lr if far from ans, dec if closer to\n",
        "# inc/dec -> decay rate \n",
        "\n",
        "# v= u + at\n",
        "# final_velocity = init_velocity + acceleration * time  \n",
        "# final_lr = init_lr + decay*epoch\n",
        "# 0 = 0.001 + decay_rate * epoch\n",
        "# decay_rate =  - init_lr / epoch # minus anyway indicates deacceleration \n",
        "\n",
        "\n",
        "# SPARSE_categorical_crossentropy -> SPARSE will ensure that only 1 prob was highest\n",
        "# categorical_crossentropy  -> just prob distribution -> multiple classes could have\n",
        "# nearby probailities! MULTIPLE CLASSES could have been detected in 1 image!!!\n",
        "# if an image has both Shaktiman and Pikachu-> we will be able to get some probab_dist for both\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82S2zwjLorBG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de6a51ea-4aff-44b6-b852-119c9262cee7"
      },
      "source": [
        "history = model.fit_generator(aug.flow(trainx, trainy, batch_size=HP_batch_size), validation_data=(testx,testy),steps_per_epoch = len(trainx)// HP_batch_size,epochs=HP_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 17s 2s/step - loss: 5.3694 - accuracy: 0.6948 - val_loss: 1.2452 - val_accuracy: 0.4526\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.4461 - accuracy: 0.9186 - val_loss: 0.8792 - val_accuracy: 0.6000\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1941 - accuracy: 0.9564 - val_loss: 0.7482 - val_accuracy: 0.6526\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.2720 - accuracy: 0.9331 - val_loss: 1.2801 - val_accuracy: 0.3789\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1354 - accuracy: 0.9602 - val_loss: 1.1814 - val_accuracy: 0.3053\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1020 - accuracy: 0.9767 - val_loss: 1.2649 - val_accuracy: 0.2421\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1846 - accuracy: 0.9593 - val_loss: 1.6620 - val_accuracy: 0.2632\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.2153 - accuracy: 0.9593 - val_loss: 0.6815 - val_accuracy: 0.6000\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0763 - accuracy: 0.9738 - val_loss: 1.1785 - val_accuracy: 0.2316\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.3825 - accuracy: 0.9494 - val_loss: 2.8228 - val_accuracy: 0.4526\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1308 - accuracy: 0.9830 - val_loss: 2.6309 - val_accuracy: 0.2526\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1310 - accuracy: 0.9826 - val_loss: 2.3131 - val_accuracy: 0.3789\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.2283 - accuracy: 0.9826 - val_loss: 0.6256 - val_accuracy: 0.7368\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1092 - accuracy: 0.9651 - val_loss: 3.0626 - val_accuracy: 0.4526\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0278 - accuracy: 0.9913 - val_loss: 2.2575 - val_accuracy: 0.1684\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1238 - accuracy: 0.9773 - val_loss: 0.7275 - val_accuracy: 0.7263\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1434 - accuracy: 0.9643 - val_loss: 1.2194 - val_accuracy: 0.4105\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0524 - accuracy: 0.9744 - val_loss: 1.7893 - val_accuracy: 0.4421\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0908 - accuracy: 0.9732 - val_loss: 2.3292 - val_accuracy: 0.3474\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.2449 - accuracy: 0.9716 - val_loss: 0.2043 - val_accuracy: 0.9263\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1536 - accuracy: 0.9651 - val_loss: 1.4332 - val_accuracy: 0.5053\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0923 - accuracy: 0.9738 - val_loss: 1.0680 - val_accuracy: 0.6632\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1254 - accuracy: 0.9767 - val_loss: 3.8240 - val_accuracy: 0.4737\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1127 - accuracy: 0.9709 - val_loss: 1.7406 - val_accuracy: 0.5684\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.1038 - accuracy: 0.9680 - val_loss: 2.0661 - val_accuracy: 0.5368\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.2538 - accuracy: 0.9709 - val_loss: 3.8921 - val_accuracy: 0.4947\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0332 - accuracy: 0.9855 - val_loss: 0.4022 - val_accuracy: 0.8211\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1096 - accuracy: 0.9826 - val_loss: 1.8827 - val_accuracy: 0.6316\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0446 - accuracy: 0.9886 - val_loss: 0.3446 - val_accuracy: 0.8526\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0497 - accuracy: 0.9855 - val_loss: 0.7483 - val_accuracy: 0.8316\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1094 - accuracy: 0.9942 - val_loss: 0.1671 - val_accuracy: 0.8947\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1190 - accuracy: 0.9767 - val_loss: 0.7601 - val_accuracy: 0.8105\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1167 - accuracy: 0.9709 - val_loss: 0.3346 - val_accuracy: 0.8316\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 21s 2s/step - loss: 0.1404 - accuracy: 0.9593 - val_loss: 0.3187 - val_accuracy: 0.9158\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.2343 - accuracy: 0.9767 - val_loss: 0.1218 - val_accuracy: 0.9368\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0659 - accuracy: 0.9797 - val_loss: 0.4733 - val_accuracy: 0.9368\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0960 - accuracy: 0.9622 - val_loss: 0.3241 - val_accuracy: 0.8842\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0479 - accuracy: 0.9826 - val_loss: 0.0594 - val_accuracy: 0.9579\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0768 - accuracy: 0.9884 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0200 - accuracy: 0.9886 - val_loss: 0.1245 - val_accuracy: 0.9789\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0296 - accuracy: 0.9911 - val_loss: 0.2569 - val_accuracy: 0.9579\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0369 - accuracy: 0.9915 - val_loss: 0.1335 - val_accuracy: 0.9789\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0865 - accuracy: 0.9851 - val_loss: 0.4462 - val_accuracy: 0.9263\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.3110 - accuracy: 0.9574 - val_loss: 0.1369 - val_accuracy: 0.9474\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1672 - accuracy: 0.9651 - val_loss: 0.9170 - val_accuracy: 0.7789\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1223 - accuracy: 0.9651 - val_loss: 0.4840 - val_accuracy: 0.8211\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0216 - accuracy: 0.9942 - val_loss: 0.6349 - val_accuracy: 0.8000\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0069 - accuracy: 0.9971 - val_loss: 0.9702 - val_accuracy: 0.8105\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0045 - accuracy: 0.9972 - val_loss: 0.5084 - val_accuracy: 0.8842\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0531 - accuracy: 0.9851 - val_loss: 0.0951 - val_accuracy: 0.9474\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0200 - accuracy: 0.9884 - val_loss: 0.0898 - val_accuracy: 0.9579\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0163 - accuracy: 0.9942 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0113 - accuracy: 0.9972 - val_loss: 0.0554 - val_accuracy: 0.9789\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 0.9789\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0744 - val_accuracy: 0.9789\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0146 - accuracy: 0.9913 - val_loss: 0.2287 - val_accuracy: 0.9579\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0205 - accuracy: 0.9886 - val_loss: 0.0375 - val_accuracy: 0.9895\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1030 - accuracy: 0.9673 - val_loss: 0.0715 - val_accuracy: 0.9789\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0501 - accuracy: 0.9858 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0583 - accuracy: 0.9884 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0052 - accuracy: 0.9971 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0253 - accuracy: 0.9913 - val_loss: 3.5511e-04 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0118 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0140 - accuracy: 0.9971 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 0.9895\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0188 - accuracy: 0.9942 - val_loss: 0.0478 - val_accuracy: 0.9789\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 7.0843e-04 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.9895\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 0.9895\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0303 - val_accuracy: 0.9895\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 6.7300e-04 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 0.9895\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 20s 2s/step - loss: 0.0028 - accuracy: 0.9971 - val_loss: 0.0265 - val_accuracy: 0.9895\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 8.3290e-04 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 0.9895\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 7.6287e-04 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 0.9895\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0023 - accuracy: 0.9971 - val_loss: 0.0240 - val_accuracy: 0.9895\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 5.6960e-04 - accuracy: 1.0000 - val_loss: 0.0252 - val_accuracy: 0.9895\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0048 - accuracy: 0.9971 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0079 - accuracy: 0.9970 - val_loss: 0.0684 - val_accuracy: 0.9895\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0689 - accuracy: 0.9886 - val_loss: 0.0427 - val_accuracy: 0.9789\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0039 - accuracy: 0.9971 - val_loss: 0.0683 - val_accuracy: 0.9895\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0053 - accuracy: 0.9971 - val_loss: 0.0810 - val_accuracy: 0.9789\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 0.9789\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0040 - accuracy: 0.9971 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 2.8031e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0321 - accuracy: 0.9942 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.0258 - val_accuracy: 0.9895\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0338 - accuracy: 0.9884 - val_loss: 0.0845 - val_accuracy: 0.9789\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0084 - accuracy: 0.9940 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0625 - accuracy: 0.9884 - val_loss: 0.0255 - val_accuracy: 0.9895\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0894 - accuracy: 0.9680 - val_loss: 0.0819 - val_accuracy: 0.9789\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1637 - accuracy: 0.9797 - val_loss: 0.0755 - val_accuracy: 0.9789\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0525 - accuracy: 0.9855 - val_loss: 0.1613 - val_accuracy: 0.9579\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.1049 - accuracy: 0.9651 - val_loss: 0.0426 - val_accuracy: 0.9895\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0140 - accuracy: 0.9971 - val_loss: 2.4645 - val_accuracy: 0.7895\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0190 - accuracy: 0.9971 - val_loss: 3.5351 - val_accuracy: 0.8947\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0630 - accuracy: 0.9886 - val_loss: 1.3181 - val_accuracy: 0.8947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyBmvHBdmDhe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "991d8a62-5f39-40dc-a640-47aafb411af1"
      },
      "source": [
        "# Exceptions -> symptoms not cause -> read your code again -> fix logical errors\n",
        "import pickle\n",
        "HP_model_path = 'bin/model/myModel.h5'\n",
        "HP_binarized_labels = 'bin/labels/labels.bin'\n",
        "model.save(HP_model_path)\n",
        "f = open(HP_binarized_labels, 'wb') # our integer is represent in binary format!\n",
        "f.write(pickle.dumps(lb))\n",
        "f.close()\n",
        "print(lb.classes_)\n",
        "# end of train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dogs' 'pikachu' 'shaktiman']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J7Txx7qM2Yv"
      },
      "source": [
        "# Beginning of Score.py\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import imutils\n",
        "import cv2\n",
        "import os\n",
        "from keras.preprocessing.image import img_to_array \n",
        "from keras.models import load_model \n",
        "\n",
        "# Load the model and binarized labels\n",
        "HP_model_path = 'bin/model/myModel.h5'\n",
        "HP_binarized_labels = 'bin/labels/labels.bin'\n",
        "# assume model is already loaded in model variable\n",
        "f = open(HP_binarized_labels, 'rb')\n",
        "mylabels = f.read()\n",
        "binarized_labels = pickle.loads(mylabels)\n",
        "\n",
        "# model.load() should be written\n",
        "# model.compile() -> else model will not work!!! \n",
        "\n",
        "HP_test_img = '/content/test/pikadoggie2.jpg'\n",
        "img = cv2.imread(HP_test_img)\n",
        "copy_img = img.copy()\n",
        "img = cv2.resize(img, (96,96))\n",
        "img = (img*1.0)/255.0\n",
        "# this is a single image, not a sequence of images \n",
        "\n",
        "# i will need convert it to the form which my model can accept \n",
        "# MODEL expecting a sequence of images, not a single image \n",
        "# Model is expecting (None, 96, 96, 32) -> None indicates that data is NOT participating in DL\n",
        "# our current shape is (96,96,32)\n",
        "img_to_feed_nn = np.expand_dims(img, axis=0) # axis = 0 -> increase dim column wise on that pos\n",
        "# post this ops-> (1,96,96,32) \n",
        "\n",
        "# input is MATRIX of Matrices not a single image (matrix of pixels in 3 channels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j08C-5QqNaDS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d0fa835-906c-4a14-dd86-ef90114f39c5"
      },
      "source": [
        "img_to_feed_nn.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 96, 96, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctLB0AZnRVAJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "104adbe8-8cdd-4fcf-c1d0-2db7743a3dfa"
      },
      "source": [
        "prediction = model.predict(img_to_feed_nn)\n",
        "print(prediction)\n",
        "print(binarized_labels.classes_)\n",
        "# categorical_crossentropy -> because multiple classes prob is being checked, \n",
        "# total_classes >= sum_of_probabilities >= 0\n",
        "# categorical_crossentropy is SIGMOID for individual CLASSES \n",
        "\n",
        "# sparse categorical crossentropy-> SUM of all the probabilities is 1\n",
        "# my dog classes has only 30 samples \n",
        "# i should have had at least 200 samples of EACH class\n",
        "\n",
        "# This equation y=mx+c, is more biased towards the classes\n",
        "# that were sufficiently represented\n",
        "# \n",
        "\n",
        "# end of Score.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.461524e-20 1.000000e+00 6.764287e-27]]\n",
            "['dogs' 'pikachu' 'shaktiman']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5V8A-zrRUx7"
      },
      "source": [
        "# this will require at least 1000 images per class, and at least 3 classes \n",
        "# below 3000 sample data (train/test) this algo may behave unexpectedly\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPg_LAalTxHF"
      },
      "source": [
        " # BATCH NORMALIZATION FUN FACT \n",
        "    # we will be calculating -> mu and sigma for normalization \n",
        "    # parameters will be generated\n",
        "    # but in back/prop-> IS my data(images) changing? if my data is not changing\n",
        "    # then mu and sigma for that data/batch will not change!!! \n",
        "    # Non-TRAINABLE parameters -> that cannot be trained -> back-prop's differentiation \n",
        "    # will NOT change their value! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26PFtAS1qvMd"
      },
      "source": [
        "# https://1drv.ms/u/s!AhM-uOEWdqAeqG9rJT9XKZp62sCj?e=hl5sKg\n",
        "# https://1drv.ms/u/s!AhM-uOEWdqAexWFgIl-sNiHnq0JM?e=V3cG4m\n",
        "\n",
        "# Pattern Recognition: pattern recognition by Sergios Theodoridis, Konstantinos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU4d4M3LqiqM"
      },
      "source": [
        "# CONVOLUTIONAL neural networks + Recurring Networks + Lot of unknown stuff = Brain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvpPcQJpjFTl"
      },
      "source": [
        "# Color theory: https://www.springer.com/gp/book/9780792399285\n",
        "\n",
        "# Concious / Unconcious -> myth \n",
        "# Cerebrum, Cerebellum, Corpus callosum, medulla -> Biologist \n",
        "# biologist and neurologist -> CLASSFIED brains into activities\n",
        "# \n",
        "# They are now proven to be partial knowledges!\n",
        "\n",
        "# Optical nerve was working fine but postoral lobe responsible for\n",
        "# vision was damaged \n",
        "\n",
        "# Neurosurgeons -> clipped optical nerve and connected to a different segment of brain\n",
        "# within matter of months, BRAIN taught itself to SEE from the area where it was not supposed\n",
        "# to! \n",
        "\n",
        "# BRAIN -> Weights and BIASES and unknown parameters! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwD_XpmOPNxe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0bf2ffbd-b495-42da-ba54-5bc6bf323283"
      },
      "source": [
        "# pandavgg model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_43 (Conv2D)           (None, 96, 96, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 96, 96, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 96, 96, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1024)              4195328   \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 3075      \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 5,365,123\n",
            "Trainable params: 5,363,267\n",
            "Non-trainable params: 1,856\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwp0Bu5Iqfmg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a909a243-fd54-4c5a-9c8e-919b41bfc10a"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "model = VGG16()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 6s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltNfJ6RKrOZ6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}